{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/glebdrozdov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from importlib import reload\n",
    "\n",
    "import base64\n",
    "import csv\n",
    "import gzip\n",
    "import zlib\n",
    "\n",
    "from collections import namedtuple\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACE_NUM = 1000\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%H:%M:%S')\n",
    "\n",
    "def trace(items_num, trace_num=TRACE_NUM):\n",
    "    if items_num % trace_num == 0: logging.info(\"Complete items %05d\" % items_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def bs_parsertext(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "html2text = bs_parsertext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_tokenizer(text):\n",
    "    word = str()\n",
    "    for symbol in text:\n",
    "        if symbol.isalnum(): word += symbol\n",
    "        elif word:\n",
    "            yield word\n",
    "            word = str()\n",
    "    if word : yield word\n",
    "\n",
    "PYMORPHY_CACHE = {}\n",
    "MORPH = None\n",
    "def get_lemmatizer():\n",
    "    import pymorphy2\n",
    "    global MORPH\n",
    "    if MORPH is None: MORPH = pymorphy2.MorphAnalyzer()\n",
    "    return MORPH\n",
    "\n",
    "def pymorphy_tokenizer(text):\n",
    "    global PYMORPHY_CACHE\n",
    "    for word in easy_tokenizer(text):\n",
    "        word_hash = hash(word)\n",
    "        if word_hash not in PYMORPHY_CACHE:\n",
    "            PYMORPHY_CACHE[word_hash] = get_lemmatizer().parse(word)[0].normal_form            \n",
    "        yield PYMORPHY_CACHE[word_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html2word(raw_html, to_text=html2text, tokenizer=pymorphy_tokenizer):\n",
    "    return tokenizer(raw_html.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocItem = namedtuple('DocItem', ['doc_id', 'is_spam', 'url', 'page_text'])\n",
    "\n",
    "def load_csv(input_file_name):    \n",
    "    with gzip.open(input_file_name, 'rt', encoding='utf-8') if input_file_name.endswith('gz') else open(input_file_name)  as input_file:            \n",
    "        headers = input_file.readline()\n",
    "        \n",
    "        for i, line in enumerate(input_file):\n",
    "            trace(i)\n",
    "            parts = line.strip().split('\\t')\n",
    "            url_id = int(parts[0])                                        \n",
    "            mark = bool(int(parts[1]))   \n",
    "            url = parts[2]\n",
    "            pageInb64 = parts[3]\n",
    "            html_data = base64.b64decode(pageInb64).decode('utf-8', errors=\"ignore\")\n",
    "            page_text = list(html2word(html_data))\n",
    "            page_text = \" \".join(str(x) for x in page_text)\n",
    "            yield DocItem(url_id, mark, url, page_text)\n",
    "        trace(i, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:29:16 INFO:Complete items 00000\n",
      "02:29:16 INFO:Loading dictionaries from /anaconda3/lib/python3.6/site-packages/pymorphy2_dicts/data\n",
      "02:29:16 INFO:format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n",
      "02:30:00 INFO:Complete items 01000\n",
      "02:30:23 INFO:Complete items 02000\n",
      "02:30:49 INFO:Complete items 03000\n",
      "02:31:12 INFO:Complete items 04000\n",
      "02:31:34 INFO:Complete items 05000\n",
      "02:31:54 INFO:Complete items 06000\n",
      "02:32:19 INFO:Complete items 07000\n",
      "02:32:20 INFO:Complete items 07043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 51s, sys: 2.46 s, total: 2min 53s\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TRAIN_DATA_FILE  = 'kaggle_train_data_tab_new.csv.gz'\n",
    "\n",
    "train_docs = list(load_csv(TRAIN_DATA_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:44:55 INFO:Complete items 00000\n",
      "17:45:21 INFO:Complete items 01000\n",
      "17:45:40 INFO:Complete items 02000\n",
      "17:45:59 INFO:Complete items 03000\n",
      "17:46:15 INFO:Complete items 04000\n",
      "17:46:31 INFO:Complete items 05000\n",
      "17:46:47 INFO:Complete items 06000\n",
      "17:47:01 INFO:Complete items 07000\n",
      "17:47:14 INFO:Complete items 08000\n",
      "17:47:26 INFO:Complete items 09000\n",
      "17:47:44 INFO:Complete items 10000\n",
      "17:47:56 INFO:Complete items 11000\n",
      "17:48:08 INFO:Complete items 12000\n",
      "17:48:19 INFO:Complete items 13000\n",
      "17:48:32 INFO:Complete items 14000\n",
      "17:48:45 INFO:Complete items 15000\n",
      "17:48:59 INFO:Complete items 16000\n",
      "17:48:59 INFO:Complete items 16038\n"
     ]
    }
   ],
   "source": [
    "TEST_DATA_FILE  = 'kaggle_test_data_tab_new.csv.gz'\n",
    "\n",
    "test_docs = list(load_csv(TEST_DATA_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train_docs)\n",
    "X_train = df_train.loc[:, df_train.columns != 'is_spam']\n",
    "y_train = df_train.is_spam\n",
    "df_test = pd.DataFrame(test_docs)\n",
    "X_test = df_test.loc[:, df_test.columns != 'is_spam']\n",
    "y_test = df_test.is_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(docs, predicted_labels):\n",
    "    with open('my_submission.csv' , 'w') as fout:\n",
    "        fout.write(\"Id,Prediction\\n\")\n",
    "        for doc_id, pred in zip(docs['doc_id'].values, predicted_labels):\n",
    "            res = 1 if pred else 0\n",
    "            fout.write(\"%d,%d\\n\" % (doc_id, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_f_metric(true_labels, predicted_labels):\n",
    "    from sklearn.metrics import f1_score\n",
    "    return f1_score(true_labels, predicted_labels, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = CountVectorizer(stop_words = stopwords.words('russian') + stopwords.words('english'),\n",
    "                                         ngram_range = (1, 2))\n",
    "        self.clf = MultinomialNB()\n",
    "\n",
    "    def predict(self, data):\n",
    "        test_trans = self.vectorizer.transform(data)\n",
    "        return self.clf.predict(test_trans)\n",
    "    \n",
    "    def train(self, train_X, train_y):\n",
    "        train_trans = self.vectorizer.fit_transform(train_X)\n",
    "        #train_trans = self.vectorizer.transform(train_X)\n",
    "        self.clf.fit(train_trans, train_y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XgBoostClassifier:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(ngram_range = (1, 2), stop_words = stopwords.words('russian') + stopwords.words('english'), \n",
    "                                                            min_df = 0.02)\n",
    "        self.bst = None\n",
    "\n",
    "    def predict(self, data):\n",
    "        test_trans = self.vectorizer.transform(data)\n",
    "        dtest = xgb.DMatrix(test_trans)\n",
    "        return self.bst.predict(dtest)\n",
    "    \n",
    "    def train(self, train_X, train_y):\n",
    "        params = {\n",
    "            'max_depth': 4, \n",
    "            'eta': 0.2, \n",
    "            'silent': 1,\n",
    "            'objective': 'multi:softmax', \n",
    "            'num_class': 2\n",
    "        }\n",
    "        rounds = 500\n",
    "        train_trans = self.vectorizer.fit_transform(train_X)\n",
    "        dtrain = xgb.DMatrix(train_trans, label=train_y)\n",
    "        self.bst = xgb.train(params, dtrain, rounds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifier:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(ngram_range = (1, 3),\n",
    "                                          stop_words = stopwords.words('russian') + stopwords.words('english'))\n",
    "        self.clf = SGDClassifier(verbose=True)\n",
    "    \n",
    "    def train(self, train_X, train_y):\n",
    "        train_trans = self.vectorizer.fit_transform(train_X)\n",
    "        self.clf.fit(train_trans, train_y)\n",
    "    \n",
    "    def train_all(self, train_all, train_X, train_y):\n",
    "        self.vectorizer.fit(train_all)\n",
    "        train_trans = self.vectorizer.transform(train_X)\n",
    "        self.clf.fit(train_trans, train_y)\n",
    "    \n",
    "    def predict(self, data):\n",
    "        test_trans = self.vectorizer.transform(data)\n",
    "        return self.clf.predict(test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(classifier, train_X, train_y, test_X):\n",
    "    classifier.train(train_X, train_y)\n",
    "    preds = classifier.predict(test_X)\n",
    "    print(count_f_metric(y_test, preds))\n",
    "    write_to_file(X_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 36.44, NNZs: 16602, Bias: 0.103635, T: 7044, Avg. loss: 0.130494\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 31.56, NNZs: 16602, Bias: 0.099918, T: 14088, Avg. loss: 0.054527\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 30.15, NNZs: 16605, Bias: 0.077696, T: 21132, Avg. loss: 0.045364\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 29.25, NNZs: 16605, Bias: 0.044421, T: 28176, Avg. loss: 0.040523\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 28.86, NNZs: 16605, Bias: 0.044470, T: 35220, Avg. loss: 0.038382\n",
      "Total training time: 0.07 seconds.\n",
      "0.5046449279880292\n"
     ]
    }
   ],
   "source": [
    "make_predictions(SVMClassifier(), X_train['page_text'], y_train.values,  X_test['page_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 53.67, NNZs: 5332726, Bias: -0.143150, T: 7044, Avg. loss: 0.113190\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 42.97, NNZs: 7621329, Bias: -0.046632, T: 14088, Avg. loss: 0.031933\n",
      "Total training time: 0.92 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 39.81, NNZs: 8647101, Bias: -0.011756, T: 21132, Avg. loss: 0.023905\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 38.08, NNZs: 9172913, Bias: -0.011109, T: 28176, Avg. loss: 0.019014\n",
      "Total training time: 1.91 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 37.38, NNZs: 9452634, Bias: 0.003133, T: 35220, Avg. loss: 0.017070\n",
      "Total training time: 2.41 seconds.\n",
      "0.5030862273209052\n"
     ]
    }
   ],
   "source": [
    "make_predictions(SVMClassifier(), X_train['page_text'], y_train.values,  X_test['page_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/anaconda3/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5135606958039778\n"
     ]
    }
   ],
   "source": [
    "make_predictions(XgBoostClassifier(), X_train['page_text'], y_train,  X_test['page_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
